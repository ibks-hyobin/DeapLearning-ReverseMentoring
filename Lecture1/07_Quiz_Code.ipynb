{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from data import fig\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Generating Dataset ====== #\n",
    "num_data = 2400\n",
    "x1 = np.random.rand(num_data) * 10\n",
    "x2 = np.random.rand(num_data) * 10\n",
    "e = np.random.normal(0, 0.5, num_data)\n",
    "X = np.array([x1, x2]).T\n",
    "y = 2*np.sin(x1) + np.log(0.5*x2**2) + e\n",
    "\n",
    "# ====== Split Dataset into Train, Validation, Test ======#\n",
    "train_X, train_Y = X[:1600, :], y[:1600]\n",
    "val_X, val_Y = X[1600:2000, :], y[1600:2000]\n",
    "test_X, test_Y = X[2000:, :], y[2000:]\n",
    "\n",
    "# ====== Visualize Each Dataset ====== #\n",
    "fig.show_3dgraph(train_X, train_Y, val_X, val_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = torch.Tensor(train_X), torch.Tensor(train_Y).view(len(train_Y),-1)\n",
    "val_x, val_y = torch.Tensor(val_X), torch.Tensor(val_Y).view(len(val_Y),-1)\n",
    "test_x, test_y = torch.Tensor(test_X), torch.Tensor(test_Y).view(len(test_Y),-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델을 구축하고 학습해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(MLPModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=2, out_features=200)\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        output = self.linear2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model  = nn.Linear(2, 1, bias=True)\n",
    "# model = LinearModel()\n",
    "model = MLPModel()\n",
    "print('{} parameters'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, cost_func, optimizer, epoch, log_interval=5):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    pred_train = []\n",
    "    pred_val = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        output = model(data)\n",
    "        pred_train.extend(output.reshape(-1).tolist())\n",
    "        cost = cost_func(output, target)\n",
    "        train_loss += cost\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "    return train_loss, pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, cost_func) :\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader :\n",
    "            output = model(data)\n",
    "            prediction.extend(output.reshape(-1).tolist())\n",
    "            test_loss += cost_func(output, target)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "            \n",
    "    \n",
    "    return test_loss, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_dataset = TensorDataset(train_x.float(), train_y.float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "validation_dataset = TensorDataset(val_x.float(), val_y.float())\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_x.float(), test_y.float())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "log_interval = 10\n",
    "list_epoch = []\n",
    "list_train_loss = []\n",
    "list_val_loss = []\n",
    "\n",
    "for epoch in range(0, epochs + 1):\n",
    "    train_loss, pred_y = train(train_loader, model, \n",
    "                            cost_func, optimizer, epoch, log_interval)\n",
    "    val_loss, pred_val = test(validation_loader, model, cost_func)\n",
    "    \n",
    "    list_epoch.append(epoch)\n",
    "    list_train_loss.append(train_loss)\n",
    "    list_val_loss.append(val_loss)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        \n",
    "        print('Epoch: {}  Train set: Average loss: {:.4f}'.format(epoch, train_loss))\n",
    "        print('\\t  Test set: Average loss: {:.4f}'.format(val_loss))\n",
    "        \n",
    "        fig.show_3dgraph(train_X, train_Y, train_X, np.array(pred_y), val_X, np.array(pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, pred_y = test(test_loader, model, cost_func)\n",
    "print('\\t  Test set: Average loss: {:.4f}'.format(test_loss))\n",
    "fig.show_2dgragh(test_X, test_Y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss graph\n",
    "\n",
    "학습 과정에서 기록했던 train_loss와 val_loss를 그려봅시다.   \n",
    "Loss가 꾸준히 줄어드는지 확인하고 val_loss가 증가하기 시킨다면 그 이상의 학습은 점점 모델의 성능을 떨어뜨림을 뜻합니다.(overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "# ====== Loss Fluctuation ====== #\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
    "ax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_ylim(0, 0.1)\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "ax1.set_title('epoch vs loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
