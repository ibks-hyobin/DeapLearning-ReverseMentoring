# deeplearning 역멘토링
2020 ibksystem 플랫폼사업팀 딥러닝 역멘토링 컨텐츠입니다.

## Lecture 1

### 1.1 Machine Learning Introduction

* Machine Learning의 정의
* Machine Learning의 분류
  * 지도학습 (supervised learning)
    * KNN classification
    * linear regression
  * 비지도학습(unsupervised learning)
    * K-means clustering

### 1.2 Linear Regression
* Gradient Descent
  * Learning Rate
* Overfitting
  * Regularization
  * Early stopping
    
### 1.3 Gradient Descent Optimization Algorithms
* Batch Gradient Descent
* Stochastic Gradient Descent (SGD)
* NAG
* Momentum
* Adagrad
* Rmsprop
* Adam

### 1.4 Binary Classification
* Logistic Regression
* Cross-Entropy

### 1.5 Multinomial Classification
* Softmax

### 1.6 실습 코드
* [01.Python Library Tutorial](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/01.Python%20Library%20Tutorial%20(numpy%2Cmatplotlib).ipynb)
* [02.Linear Regression with Python](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/02.Linear%20Regression.ipynb)
* [03.PyTorch Tutorial](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/03_Pytorch_Tutorial.ipynb)
* [04.Linear Regression with PyTorch](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/04_Linear_Regression_Models.ipynb)

### 참고 자료
* [stanford university cs231 Lecture 2, Lecture 3](http://cs231n.stanford.edu/2018/syllabus.html)
* [머신러닝, 1시간으로 입문하기](https://www.youtube.com/watch?v=j3za7nv7RfI&t=2047s)
* [머신러닝의 기초 - 선형 회귀 한 번에 제대로 이해하기](https://www.youtube.com/watch?v=ve6gtpZV83E&t=1619s)
* [Gradient Descent Optimization Algorithms 정리](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)
* [Regularization(Overfitting해결책)](https://m.blog.naver.com/laonple/220527647084)


## Lecture 2

## Lecture 3
