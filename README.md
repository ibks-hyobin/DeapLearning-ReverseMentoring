# deeplearning 역멘토링
2020 ibksystem 플랫폼사업팀 딥러닝 역멘토링 컨텐츠입니다.

## Lecture 1

### 1.1 Machine Learning Introduction

* Machine Learning의 정의
* Machine Learning의 분류
  * 지도학습 (supervised learning)
    * KNN classification
    * linear regression
  * 비지도학습(unsupervised learning)
    * K-means clustering

### 1.2 Linear Regression
* Gradient Descent
  * Learning Rate
* Overfitting
  * Regularization
  * Early stopping
    
### 1.3 Gradient Descent Optimization Algorithms
* Batch Gradient Descent
* Stochastic Gradient Descent (SGD)
* NAG
* Momentum
* Adagrad
* Rmsprop
* Adam

### 1.4 Binary Classification
* Logistic Regression
* Cross-Entropy

### 1.5 Multinomial Classification
* Softmax

### 1.6 실습 코드
* [01.Python Library Tutorial](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/Lecture1/01_Python_Library_Tutorial%20(numpy%2Cmatplotlib).ipynb)
* [02.Linear Regression with Python](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/Lecture1/02_Linear_Regression.ipynb)
* [03.PyTorch Tutorial](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/Lecture1/03_Pytorch_Tutorial.ipynb)
* [04.PyTorch Variable Autograd](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/Lecture1/04_variable_autograd.ipynb)
* [05.Linear Regression with PyTorch](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/Lecture1/05_Linear_Regression_Models.ipynb)
* [06.Quiz(homework)](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/Lecture1/06_Quiz.ipynb)

### 참고 자료
* [stanford university cs231 Lecture 2, Lecture 3](http://cs231n.stanford.edu/2018/syllabus.html)
* [머신러닝, 1시간으로 입문하기](https://www.youtube.com/watch?v=j3za7nv7RfI&t=2047s)
* [머신러닝의 기초 - 선형 회귀 한 번에 제대로 이해하기](https://www.youtube.com/watch?v=ve6gtpZV83E&t=1619s)
* [Gradient Descent Optimization Algorithms 정리](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)
* [Building a Logistic Regression in Python](https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24)
* [PyTorch로 시작하는 딥러닝 입문](https://wikidocs.net/55580)


## Lecture 2
### 2.1 Introduction to Neural Networks
* History of ANN
* Universal approximation theorem

### 2.2 Multi-layer Perceptrons (MLP)
* XOR problem
* activation functions
  * gradient vanishing problem
* Feed-forward Neural Network
* Error Backpropagation

### 2.3 Regularization
* L1 regularization
* L2 regularization
* dropout
* batch normalization

### 2.4 Weights Initialization
* He initialization
* Xavier Initialization

### 2.5 실습 코드
* [0.1 Multi-Layer Perceptrons](https://github.com/ibks-hyobin/deeplearning-reverseMentoring/blob/master/Lecture1/07_Quiz_Code.ipynb)
* [0.2 MNIST data classification with MLP]

### 참고 자료
* [딥러닝 역사](http://blog.naver.com/PostView.nhn?blogId=windowsub0406&logNo=220883022888)
* [stanford university cs231 Lecture 4](http://aikorea.org/cs231n/optimization-2/)
* [Activation Functions](https://deepestdocs.readthedocs.io/en/latest/002_deep_learning_part_1/0024/)
* [Dropout](https://deepestdocs.readthedocs.io/en/latest/004_deep_learning_part_2/0041/)
* [Batch Normalization](https://sacko.tistory.com/44)

## Lecture 3
### 3.1 Convolution Neural Network


### 3.2 CNN Architectures
* AlexNet
* VGG
* GoogLeNet
* ResNet
* etc

### 3.3 GPU

### 3.4 실습 코드

## PyTorch Sources
* 참고 자료 : https://github.com/gyunggyung/PyTorch
* Pytorch example : https://github.com/pytorch/examples
* Pytorch tutorial : https://github.com/pytorch/tutorials
* Pytorch documentation : https://pytorch.org/docs/master/
